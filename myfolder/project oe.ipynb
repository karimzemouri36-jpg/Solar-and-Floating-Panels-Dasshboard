{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2c8caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'newdata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m shale_gas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewdata.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, shale_gas\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kirit\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\kirit\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\kirit\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\kirit\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\kirit\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'newdata.csv'"
     ]
    }
   ],
   "source": [
    "# First, let's reload the data with proper analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the data\n",
    "shale_gas = pd.read_csv('newdata.csv')\n",
    "\n",
    "print(\"Dataset shape:\", shale_gas.shape)\n",
    "print(\"\\nDataset columns:\")\n",
    "print(shale_gas.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(shale_gas.head())\n",
    "print(\"\\nData types:\")\n",
    "print(shale_gas.dtypes)\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(shale_gas.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(shale_gas.isnull().sum())\n",
    "\n",
    "# Check class distribution in Category\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(shale_gas['Category'].value_counts())\n",
    "print(\"\\nCategory proportions:\")\n",
    "print(shale_gas['Category'].value_counts(normalize=True))\n",
    "\n",
    "# Enhanced EDA function\n",
    "def detailed_eda(df):\n",
    "    print(\"=\"*80)\n",
    "    print(\"ENHANCED EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Target variable analysis\n",
    "    print(\"\\n1. TARGET VARIABLE (EUR) ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Mean EUR: {df['EUR'].mean():.2f}\")\n",
    "    print(f\"Median EUR: {df['EUR'].median():.2f}\")\n",
    "    print(f\"Std EUR: {df['EUR'].std():.2f}\")\n",
    "    print(f\"Min EUR: {df['EUR'].min():.2f}\")\n",
    "    print(f\"Max EUR: {df['EUR'].max():.2f}\")\n",
    "    print(f\"Range EUR: {df['EUR'].max() - df['EUR'].min():.2f}\")\n",
    "    \n",
    "    # 2. Correlation with EUR\n",
    "    print(\"\\n2. TOP CORRELATIONS WITH EUR:\")\n",
    "    print(\"-\" * 40)\n",
    "    corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "    eur_corr = corr_matrix['EUR'].sort_values(ascending=False)\n",
    "    for feature, corr in eur_corr.items():\n",
    "        if feature != 'EUR':\n",
    "            print(f\"{feature:20s}: {corr:7.3f}\")\n",
    "    \n",
    "    # 3. Feature analysis by category\n",
    "    print(\"\\n3. FEATURE COMPARISON BY CATEGORY:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if col != 'EUR':\n",
    "            new_design_mean = df[df['Category'] == 'New_Design'][col].mean()\n",
    "            vintage_mean = df[df['Category'] == 'Vintage'][col].mean()\n",
    "            print(f\"{col:20s}: New Design={new_design_mean:7.2f}, Vintage={vintage_mean:7.2f}, Diff={(new_design_mean-vintage_mean):7.2f}\")\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "corr_matrix = detailed_eda(shale_gas)\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. EUR distribution\n",
    "plt.subplot(3, 3, 1)\n",
    "sns.histplot(data=shale_gas, x='EUR', hue='Category', kde=True)\n",
    "plt.title('EUR Distribution by Category')\n",
    "\n",
    "# 2. Top correlated features with EUR\n",
    "top_features = corr_matrix['EUR'].sort_values(ascending=False).index[1:6]\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(3, 3, i+2)\n",
    "    sns.scatterplot(data=shale_gas, x=feature, y='EUR', hue='Category', alpha=0.6)\n",
    "    plt.title(f'{feature} vs EUR')\n",
    "    plt.tight_layout()\n",
    "\n",
    "# 3. Correlation heatmap\n",
    "plt.subplot(3, 3, 7)\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots for important features by category\n",
    "important_features = ['Lateral Length', 'Proppant Loading', 'Porosity', 'Injection Rate', 'Thickness']\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(important_features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(data=shale_gas, x='Category', y=feature)\n",
    "    plt.title(f'{feature} by Category')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Encode categorical variable\n",
    "    le = LabelEncoder()\n",
    "    df_processed['Category_encoded'] = le.fit_transform(df_processed['Category'])\n",
    "    \n",
    "    # Drop the original categorical column\n",
    "    df_processed = df_processed.drop('Category', axis=1)\n",
    "    \n",
    "    return df_processed, le\n",
    "\n",
    "# Split data\n",
    "shale_gas_processed, label_encoder = preprocess_data(shale_gas)\n",
    "\n",
    "# Prepare features and target\n",
    "X = shale_gas_processed.drop('EUR', axis=1)\n",
    "y = shale_gas_processed['EUR']\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=shale_gas['Category']\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Testing set size: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Train RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Test RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Train MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Test MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'Train R2': r2_score(y_train, y_train_pred),\n",
    "        'Test R2': r2_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_test_pred, model\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = []\n",
    "predictions = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    metrics, y_pred, trained_model = evaluate_model(\n",
    "        model, X_train_scaled, X_test_scaled, y_train, y_test, name\n",
    "    )\n",
    "    results.append(metrics)\n",
    "    predictions[name] = y_pred\n",
    "    trained_models[name] = trained_model\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Feature importance analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get feature importance from Random Forest\n",
    "rf_model = trained_models['Random Forest']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualization of feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare predictions vs actual\n",
    "best_model_name = results_df.loc[results_df['Test R2'].idxmax(), 'Model']\n",
    "print(f\"\\nBest model: {best_model_name}\")\n",
    "\n",
    "# Plot predictions vs actual for best model\n",
    "y_pred_best = predictions[best_model_name]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(y_test, y_pred_best, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual EUR')\n",
    "plt.ylabel('Predicted EUR')\n",
    "plt.title(f'{best_model_name}: Predictions vs Actual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plot\n",
    "plt.subplot(1, 3, 2)\n",
    "residuals = y_test - y_pred_best\n",
    "plt.scatter(y_pred_best, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted EUR')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title(f'{best_model_name}: Residual Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of errors\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Prediction Error')\n",
    "plt.title(f'{best_model_name}: Error Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter tuning for the best model\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:\n",
    "    print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }\n",
    "        model = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    elif best_model_name == 'XGBoost':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        model = xgb.XGBRegressor(random_state=42)\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grid, cv=5, \n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1, verbose=1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best CV score: {-grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Evaluate best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"\\nTuned {best_model_name} Performance:\")\n",
    "    print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_tuned)):.4f}\")\n",
    "    print(f\"Test MAE: {mean_absolute_error(y_test, y_pred_tuned):.4f}\")\n",
    "    print(f\"Test R2: {r2_score(y_test, y_pred_tuned):.4f}\")\n",
    "\n",
    "# Feature engineering suggestions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUGGESTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create interaction features\n",
    "X_train_engineered = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_engineered = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "# Suggest potential interaction features based on domain knowledge\n",
    "interaction_suggestions = [\n",
    "    ('Lateral Length', 'Proppant Loading'),\n",
    "    ('Porosity', 'Thickness'),\n",
    "    ('Injection Rate', 'Stage Spacing'),\n",
    "    ('bbl/ft', 'Proppant Loading')\n",
    "]\n",
    "\n",
    "print(\"\\nSuggested interaction features to try:\")\n",
    "for feat1, feat2 in interaction_suggestions:\n",
    "    if feat1 in X.columns and feat2 in X.columns:\n",
    "        print(f\"- {feat1} Ã— {feat2}\")\n",
    "\n",
    "# Business insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUSINESS INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. EUR comparison between categories\n",
    "new_design_eur = shale_gas[shale_gas['Category'] == 'New_Design']['EUR'].mean()\n",
    "vintage_eur = shale_gas[shale_gas['Category'] == 'Vintage']['EUR'].mean()\n",
    "print(f\"1. Average EUR by category:\")\n",
    "print(f\"   New Design: {new_design_eur:.2f}\")\n",
    "print(f\"   Vintage: {vintage_eur:.2f}\")\n",
    "print(f\"   Difference: {new_design_eur - vintage_eur:.2f} (+{(new_design_eur/vintage_eur - 1)*100:.1f}%)\")\n",
    "\n",
    "# 2. Key drivers analysis\n",
    "print(\"\\n2. Top 5 features influencing EUR:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# 3. Optimal ranges for key features\n",
    "print(\"\\n3. Optimal ranges for top features in high-EUR wells:\")\n",
    "high_eur_threshold = shale_gas['EUR'].quantile(0.75)\n",
    "high_eur_wells = shale_gas[shale_gas['EUR'] >= high_eur_threshold]\n",
    "\n",
    "for feature in feature_importance.head(3)['feature'].tolist():\n",
    "    if feature in shale_gas.columns:\n",
    "        optimal_min = high_eur_wells[feature].min()\n",
    "        optimal_max = high_eur_wells[feature].max()\n",
    "        optimal_mean = high_eur_wells[feature].mean()\n",
    "        print(f\"   {feature}: {optimal_min:.1f} - {optimal_max:.1f} (avg: {optimal_mean:.1f})\")\n",
    "\n",
    "# Save the best model\n",
    "import joblib\n",
    "joblib.dump(trained_models[best_model_name], 'best_eur_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "print(f\"\\nBest model ({best_model_name}) saved to 'best_eur_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
